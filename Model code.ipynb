{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import BatchNormalization,Flatten,\\\n",
    "Add,Input,Dense, Dropout, Activation, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_globals():\n",
    "    global open3D_directory\n",
    "    global dataset_name_num\n",
    "    global MIF_nodes \n",
    "    global xyz_offset\n",
    "    open3D_directory = \"C:\\open3dtools\"\n",
    "    dataset_name_num = \"06.DHFR\"\n",
    "    MIF_nodes = [23,27,21]\n",
    "    xyz_offset = [-11,-10,-9]\n",
    "set_globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    path = os.path.join(r\"C:\\Users\\Linden\\GitHub\\3DQSAR\\data\\LabelledData\",dataset_name_num)\n",
    "    labels = pd.read_csv(os.path.join(path,\"pIC50.CSV\"))\n",
    "    Y = labels[\"Y\"].to_numpy()\n",
    "\n",
    "    data=[]\n",
    "\n",
    "    for entry in os.scandir(path):\n",
    "        if (entry.path.endswith(\".npy\")):\n",
    "            file = np.load(entry.path)\n",
    "            data.append(file)\n",
    "    field_1 = data[0::2]\n",
    "    field_2 = data[1::2]\n",
    "    X_1 = np.stack(field_1,0)\n",
    "    X_2 = np.stack(field_2,0)    \n",
    "    \n",
    "    return X_1, X_2, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1, X_2, Y = import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Normalise image data \n",
    "###Normalise by amax value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Shuffle\n",
    "permutation = np.random.RandomState(seed=42)\\\n",
    "                .permutation(X_1.shape[0])\n",
    "X_1 = X_1[permutation]\n",
    "X_2 = X_2[permutation]\n",
    "Y = Y[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Train test split\n",
    "train_upper = int(np.around(X_1.shape[0]*0.9,0))\n",
    "\n",
    "X_1_train = X_1[:train_upper]\n",
    "X_2_train = X_2[:train_upper]\n",
    "X_1_test = X_1[train_upper:]\n",
    "X_2_test = X_2[train_upper:]\n",
    "\n",
    "Y_train = Y[:train_upper]\n",
    "Y_test = Y[train_upper:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Train validation split\n",
    "train_upper = int(np.around(X_1_train.shape[0]*0.8,0))\n",
    "X_1_val = X_1_train[train_upper:]\n",
    "X_2_val = X_2_train[train_upper:]\n",
    "X_1_train = X_1_train[:train_upper]\n",
    "X_2_train = X_2_train[:train_upper]\n",
    "\n",
    "Y_val = Y_train[train_upper:]\n",
    "Y_train = Y_train[:train_upper]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN():\n",
    "    model = Sequential(name='CNN')\n",
    "    \n",
    "    # elu=Exponential Linear Unit, similar to leaky Relu\n",
    "    #perhaps normalise in layer 1 \n",
    "    \n",
    "    # Convolution Layers\n",
    "    inputs = Input(shape=(X_1_train.shape[1], X_1_train.shape[2],X_1_train.shape[3]))\n",
    "    model = MaxPooling2D(2)(inputs)\n",
    "    model = Conv2D(32, (2, 2), strides=(1, 1), activation='elu')(model)\n",
    "    model = MaxPooling2D(2)(model)\n",
    "    model = Conv2D(64, (2, 2), strides=(1, 1), activation='elu')(model)\n",
    "    model = MaxPooling2D(2)(model)\n",
    "    model = Conv2D(128, (2, 2), strides=(1, 1), activation='elu')(model)\n",
    "    \n",
    "\n",
    "    output = Flatten()(model)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_headed_network():\n",
    "    X_1_CNN = build_CNN()\n",
    "    X_2_CNN = build_CNN()\n",
    "    network = Add()([X_1_CNN.output,X_2_CNN.output])\n",
    "    network = Flatten()(network)\n",
    "    network = Dense(10,activation='elu')(network)\n",
    "    network = Dense(1,activation = 'linear')(network)\n",
    "    model = Model([X_1_CNN.input, X_2_CNN.input], network)\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model  \n",
    "CNN = two_headed_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 1094885888.0000 - accuracy: 0.0000e+00 - val_loss: 84060656.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 145883872.0000 - accuracy: 0.0000e+00 - val_loss: 105647088.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 116862824.0000 - accuracy: 0.0000e+00 - val_loss: 11215502.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 3516611.5000 - accuracy: 0.0000e+00 - val_loss: 111555.2656 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 46.0109 - accuracy: 0.0000e+00 - val_loss: 45.6560 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 46.0281 - accuracy: 0.0000e+00 - val_loss: 45.6690 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 46.0398 - accuracy: 0.0000e+00 - val_loss: 45.6779 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 46.0477 - accuracy: 0.0000e+00 - val_loss: 45.6841 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 46.0532 - accuracy: 0.0000e+00 - val_loss: 45.6882 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 46.0570 - accuracy: 0.0000e+00 - val_loss: 45.6911 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 46.0595 - accuracy: 0.0000e+00 - val_loss: 45.6930 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 46.0612 - accuracy: 0.0000e+00 - val_loss: 45.6942 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 46.0623 - accuracy: 0.0000e+00 - val_loss: 45.6950 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 46.0630 - accuracy: 0.0000e+00 - val_loss: 45.6955 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 46.0634 - accuracy: 0.0000e+00 - val_loss: 45.6958 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 46.0636 - accuracy: 0.0000e+00 - val_loss: 45.6959 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 46.0637 - accuracy: 0.0000e+00 - val_loss: 45.6960 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 46.0637 - accuracy: 0.0000e+00 - val_loss: 45.6959 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 46.0637 - accuracy: 0.0000e+00 - val_loss: 45.6958 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 46.0636 - accuracy: 0.0000e+00 - val_loss: 45.6957 - val_accuracy: 0.0000e+00\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00020: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dbc8998b50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Train \n",
    "##monitor early stopping using val_acc\n",
    "#Set min_delta to 1e-3\n",
    "monitor = EarlyStopping(monitor='val_loss',\n",
    "                        min_delta=0,\n",
    "                        patience=15,\n",
    "                        verbose=1, \n",
    "                        mode='auto',\n",
    "                        restore_best_weights=True)\n",
    "\n",
    "CNN.fit([X_1_train, X_2_train],\n",
    "        Y_train,\n",
    "        batch_size=64,\n",
    "        epochs=200, \n",
    "        validation_data=([X_1_val, X_2_val],\n",
    "                          Y_val),\n",
    "                          callbacks=[monitor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step - loss: 49.0109 - accuracy: 0.0000e+00\n",
      "Model loss:[49.010948181152344, 0.0]\n"
     ]
    }
   ],
   "source": [
    "##Test model w/ best weights on validation data\n",
    "model_loss = CNN.evaluate([X_1_test,X_2_test], Y_test)\n",
    "print(f\"Model loss:{model_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
